{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOil6UXCELm8VMCk0+h+Ai0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c0991100247/2025.07.12-PERIC/blob/main/P_ETHI_02_%E5%8F%B8%E6%B3%95%E9%99%A2%E8%A3%81%E5%88%A4%E6%9B%B8%E4%B8%AD%E7%9A%84%E6%95%91%E8%AD%B7%E6%8A%80%E8%A1%93%E5%93%A1_%E5%A4%9A%E6%A8%A1%E6%85%8B%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%81%8B%E7%94%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai # ç”¨æ–¼Gemini API\n",
        "!pip install -q -U PyMuPDF             # ç”¨æ–¼PDFæ–‡å­—æ“·å– (fitz)"
      ],
      "metadata": {
        "id": "LYQMyq3B1OMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import fitz # PyMuPDF (for PDFæ–‡å­—æ“·å–)\n",
        "from tqdm.notebook import tqdm\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from datetime import datetime # ç”¨æ–¼æ—¥æœŸè½‰æ›\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/drive') # Mount Drive if needed, but keep path general\n",
        "\n",
        "# --- æ­¥é©Ÿ 0: è¨­å®šçµ„æ…‹èˆ‡è¼‰å…¥å‡½å¼åº« ---\n",
        "\n",
        "# é‡è¦ï¼šè«‹èª¿æ•´é€™äº›è·¯å¾‘ä»¥ç¬¦åˆæ‚¨å„²å­˜æ–‡ä»¶å’Œå¸Œæœ›è¼¸å‡ºæ–‡ä»¶çš„ä½ç½®ï¼\n",
        "# å»ºè­°å°‡PDFæ–‡ä»¶ä¸Šå‚³åˆ°Colabç’°å¢ƒä¸­ï¼Œæˆ–ä½¿ç”¨Googleé›²ç«¯ç¡¬ç¢Ÿä¸¦åœ¨æ­¤è™•æŒ‡å®šæ­£ç¢ºè·¯å¾‘ã€‚\n",
        "# ç¯„ä¾‹ï¼šå¦‚æœæ‚¨å°‡PDFä¸Šå‚³åˆ° Colab çš„ /content/pdfs è³‡æ–™å¤¾\n",
        "# PDF_FOLDER_PATH = '/content/pdfs'\n",
        "# ç¯„ä¾‹ï¼šå¦‚æœæ‚¨ä½¿ç”¨ Google é›²ç«¯ç¡¬ç¢Ÿï¼Œä¸”æ–‡ä»¶åœ¨ 'My Drive/MyLegalDocs' è³‡æ–™å¤¾ä¸­\n",
        "# drive.mount('/content/drive') # å¦‚æœä½¿ç”¨é›²ç«¯ç¡¬ç¢Ÿï¼Œè«‹å–æ¶ˆè¨»è§£æ­¤è¡Œ\n",
        "# PDF_FOLDER_PATH = '/content/drive/MyDrive/MyLegalDocs'\n",
        "\n",
        "# è«‹åœ¨æ­¤è™•æŒ‡å®šæ‚¨çš„PDFæ–‡ä»¶è³‡æ–™å¤¾è·¯å¾‘\n",
        "PDF_FOLDER_PATH = '/content/drive/MyDrive/' # <-- è«‹ä¿®æ”¹æ­¤è·¯å¾‘\n",
        "\n",
        "# è™•ç†å¾Œçš„è³‡æ–™è¼¸å‡ºæª”æ¡ˆ (å°‡å„²å­˜è‡³æŒ‡å®šè·¯å¾‘)\n",
        "# å»ºè­°å„²å­˜åœ¨ Colab ç’°å¢ƒä¸­ï¼Œæˆ– Google é›²ç«¯ç¡¬ç¢Ÿä¸­çš„æŒ‡å®šä½ç½®\n",
        "# ç¯„ä¾‹ï¼šOUTPUT_JSON_PATH = '/content/processed_documents.json'\n",
        "# ç¯„ä¾‹ï¼šOUTPUT_JSON_PATH = '/content/drive/MyDrive/processed_legal_documents.json'\n",
        "\n",
        "# è«‹åœ¨æ­¤è™•æŒ‡å®šæ‚¨çš„è¼¸å‡º JSON æª”æ¡ˆè·¯å¾‘\n",
        "OUTPUT_JSON_PATH = '/content/drive/MyDrive/' # <-- è«‹ä¿®æ”¹æ­¤è·¯å¾‘\n",
        "\n",
        "# Gemini APIè™•ç†æ–‡ä»¶æ•¸é‡é™åˆ¶ (è¨­å®šç‚º None è™•ç†æ‰€æœ‰æ–‡ä»¶)\n",
        "DOCUMENT_PROCESS_LIMIT = None\n",
        "\n",
        "print(\"å‡½å¼åº«è¼‰å…¥å®Œæˆï¼Œçµ„æ…‹è·¯å¾‘å·²å®šç¾©ã€‚\")\n",
        "print(f\"è«‹ç¢ºèªä¸¦èª¿æ•´ä»¥ä¸‹è·¯å¾‘ä»¥ç¬¦åˆæ‚¨çš„éœ€æ±‚:\")\n",
        "print(f\"PDFæ–‡ä»¶é æœŸè·¯å¾‘: {PDF_FOLDER_PATH}\")\n",
        "print(f\"è™•ç†å¾Œçš„è³‡æ–™å°‡å„²å­˜è‡³: {OUTPUT_JSON_PATH}\")"
      ],
      "metadata": {
        "id": "u19Yo8Fc0Gl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- æ­¥é©Ÿ 1: å®šç¾©PDFæ–‡å­—æ“·å–å‡½å¼ ---\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str | None:\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨PyMuPDFå¾PDFæ–‡ä»¶ä¸­æ“·å–æ‰€æœ‰æ–‡å­—ã€‚\n",
        "    è™•ç†æ“·å–éç¨‹ä¸­å¯èƒ½ç™¼ç”Ÿçš„éŒ¯èª¤ã€‚\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        for page_num in range(doc.page_count):\n",
        "            page = doc.load_page(page_num)\n",
        "            text += page.get_text() # ä¸²æ¥æ‰€æœ‰é é¢çš„æ–‡å­—\n",
        "        doc.close()\n",
        "    except Exception as e:\n",
        "        print(f\"éŒ¯èª¤ï¼šå¾ {pdf_path} æ“·å–æ–‡å­—å¤±æ•—: {e}\")\n",
        "        return None\n",
        "    return text\n",
        "\n",
        "print(\"`extract_text_from_pdf` å‡½å¼å·²å®šç¾©ã€‚\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3r28eNBX0VPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- æ­¥é©Ÿ 2: å®šç¾©æ¡ˆä»¶å­—è™Ÿæ“·å–å‡½å¼ (æ­£è¦è¡¨é”å¼) ---\n",
        "\n",
        "def extract_case_number(text: str) -> str | None:\n",
        "    \"\"\"\n",
        "    å¾æ–‡ä»¶æ–‡å­—ä¸­æ“·å–å°ç£æ³•é™¢çš„æ¡ˆä»¶å­—è™Ÿ (æ¡ˆè™Ÿ)ã€‚\n",
        "    æ­£è¦åŒ–ï¼šç§»é™¤ç©ºæ ¼ï¼Œä¸¦å°‡ 'è‡º' æ›¿æ›ç‚º 'å°'ã€‚\n",
        "    \"\"\"\n",
        "    regex_pattern = r'^(?:[\\s\\S]*?)?(?P<court_name>è‡ºç£[\\u4E00-\\u9FFF\\s]{2,10}?(?:åœ°æ–¹æ³•é™¢|é«˜ç­‰æ³•é™¢|æœ€é«˜æ³•é™¢))([\\u4E00-\\u9FFF\\s]{1,15})?\\s*(?P<year>\\d{2,3})å¹´åº¦(?P<case_type>[\\u4E00-\\u9FFF]{1,5}?)å­—ç¬¬(?P<number>\\d+)è™Ÿ'\n",
        "    match = re.search(regex_pattern, text, re.MULTILINE)\n",
        "\n",
        "    if match:\n",
        "        court_name = match.group('court_name').strip()\n",
        "        year = match.group('year')\n",
        "        case_type = match.group('case_type').strip()\n",
        "        number = match.group('number')\n",
        "\n",
        "        full_extracted_id = f\"{court_name}{year}å¹´åº¦{case_type}å­—ç¬¬{number}è™Ÿ\"\n",
        "        cleaned_id = full_extracted_id.replace(' ', '').replace('è‡º', 'å°')\n",
        "        return cleaned_id\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "print(\"`extract_case_number` å‡½å¼å·²å®šç¾©ã€‚\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NQm1ZpQL0Xq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- æ­¥é©Ÿ 3: å®šç¾©ä¸»æ–‡ä»¶è™•ç†å‡½å¼ ---\n",
        "\n",
        "def process_all_documents(pdf_dir: str, output_file: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    è™•ç†æŒ‡å®šè³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶ï¼Œä¸¦å°‡çµæœå„²å­˜ç‚ºJSONã€‚\n",
        "    æ–‡ä»¶IDç›´æ¥ä½¿ç”¨PDFæª”å (ä¸å«å‰¯æª”å)ã€‚\n",
        "    \"\"\"\n",
        "    processed_documents = {}\n",
        "\n",
        "    print(f\"\\n--- é–‹å§‹è™•ç†PDFæ–‡ä»¶ï¼Œè·¯å¾‘ä¾†è‡ª: {pdf_dir} ---\")\n",
        "    if not os.path.exists(pdf_dir):\n",
        "        print(f\"è­¦å‘Š: æ‰¾ä¸åˆ°PDFè³‡æ–™å¤¾ï¼Œè·¯å¾‘ä½æ–¼ {pdf_dir}ã€‚è·³éPDFè™•ç†ã€‚\")\n",
        "        return []\n",
        "\n",
        "    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith('.pdf')]\n",
        "    print(f\"æ‰¾åˆ° {len(pdf_files)} å€‹PDFæ–‡ä»¶ã€‚\")\n",
        "\n",
        "    for filename in tqdm(pdf_files, desc=\"è™•ç†PDFæ–‡ä»¶\"):\n",
        "        pdf_path = os.path.join(pdf_dir, filename)\n",
        "        doc_content = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # æ–‡ä»¶IDç›´æ¥ä¾†è‡ªPDFæª”å (ç§»é™¤.pdfå‰¯æª”å)\n",
        "        document_id = filename[:-4].strip().replace(' ', '').replace('è‡º', 'å°')\n",
        "        document_notes = []\n",
        "\n",
        "        if doc_content:\n",
        "            # å˜—è©¦å¾å…§å®¹ä¸­æ“·å–æ¡ˆä»¶å­—è™Ÿ (ç”¨æ–¼å¾ŒçºŒGeminiåˆ†æåƒè€ƒ)\n",
        "            extracted_case_number_from_content = extract_case_number(doc_content)\n",
        "            if extracted_case_number_from_content is None:\n",
        "                document_notes.append(\"åœ¨PDFå…§å®¹ä¸­æœªèƒ½æ‰¾åˆ°æ¨™æº–æ¡ˆä»¶å­—è™Ÿã€‚\")\n",
        "            elif extracted_case_number_from_content != document_id:\n",
        "                document_notes.append(\n",
        "                    f\"æ–‡ä»¶ID '{document_id}' èˆ‡å…§å®¹æ“·å–åˆ°çš„æ¡ˆä»¶å­—è™Ÿ '{extracted_case_number_from_content}' ä¸ç¬¦ã€‚\"\n",
        "                )\n",
        "\n",
        "            # å°‡æ–‡ä»¶è³‡è¨ŠåŠ å…¥è™•ç†æ¸…å–®\n",
        "            processed_documents[document_id] = {\n",
        "                \"document_id\": document_id,\n",
        "                \"original_source_format\": \"PDF\",\n",
        "                \"original_filename\": filename,\n",
        "                \"full_text\": doc_content,\n",
        "                \"extracted_case_number_from_content\": extracted_case_number_from_content,\n",
        "                \"notes\": document_notes if document_notes else [\"æ–‡ä»¶IDä¾†è‡ªæª”åã€‚\"]\n",
        "            }\n",
        "        else:\n",
        "            print(f\"è·³éPDFæ–‡ä»¶ '{filename}'ï¼Œå› æ–‡å­—æ“·å–éŒ¯èª¤ (ä¾‹å¦‚ï¼šPDFæ ¼å¼ä¸æ­£ç¢º)ã€‚\")\n",
        "            document_notes.append(\"æ–‡å­—æ“·å–å¤±æ•—ã€‚\")\n",
        "            processed_documents[document_id] = {\n",
        "                \"document_id\": document_id,\n",
        "                \"original_source_format\": \"PDF\",\n",
        "                \"original_filename\": filename,\n",
        "                \"full_text\": None,\n",
        "                \"extracted_case_number_from_content\": None,\n",
        "                \"notes\": document_notes\n",
        "            }\n",
        "\n",
        "\n",
        "    print(f\"PDFè™•ç†å®Œæˆã€‚ç›®å‰è­˜åˆ¥å‡º {len(processed_documents)} å€‹æ–‡ä»¶ã€‚\")\n",
        "\n",
        "    final_output_data = list(processed_documents.values())\n",
        "\n",
        "    # å°‡æ•´åˆå¾Œçš„è³‡æ–™å„²å­˜è‡³JSONæª”æ¡ˆ\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(final_output_data, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"æ‰€æœ‰è™•ç†éçš„æ–‡ä»¶è³‡æ–™å·²å„²å­˜è‡³Colabç’°å¢ƒä¸­çš„ '{output_file}'ã€‚\")\n",
        "\n",
        "    return final_output_data\n",
        "\n",
        "print(\"`process_all_documents` å‡½å¼å·²å®šç¾©ã€‚\")\n",
        "\n",
        "# --- æ­¥é©Ÿ 4: åŸ·è¡Œæ–‡ä»¶è™•ç† ---\n",
        "\n",
        "# ç¢ºä¿æ‚¨çš„Googleé›²ç«¯ç¡¬ç¢Ÿå·²æ›è¼‰ï¼\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "final_documents_data = process_all_documents(PDF_FOLDER_PATH, OUTPUT_JSON_PATH)\n",
        "\n",
        "print(f\"\\n`process_all_documents` åŸ·è¡Œå®Œæˆã€‚ç¸½è¨ˆè­˜åˆ¥å‡ºç¨ç‰¹æ–‡ä»¶æ•¸: {len(final_documents_data)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "r0iAHLSs0a16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- æ­¥é©Ÿ 5: æª¢æŸ¥è¼¸å‡ºçµæœ ---\n",
        "\n",
        "if os.path.exists(OUTPUT_JSON_PATH):\n",
        "    with open(OUTPUT_JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "        processed_data = json.load(f)\n",
        "\n",
        "    print(f\"\\n--- è™•ç†å¾Œè³‡æ–™ç¯„ä¾‹ ({len(processed_data)} ç¸½æ–‡ä»¶æ•¸) ---\")\n",
        "    for i, doc in enumerate(processed_data[:5]): # é¡¯ç¤ºå‰5å€‹æ–‡ä»¶\n",
        "        print(f\"\\næ–‡ä»¶ {i+1}:\")\n",
        "        for key, value in doc.items():\n",
        "            if key == \"full_text\" and isinstance(value, str) and len(value) > 200:\n",
        "                print(f\"  {key}: {value[:200]}... (å·²æˆªæ–·)\")\n",
        "            else:\n",
        "                print(f\"  {key}: {value}\")\n",
        "    if len(processed_data) > 5:\n",
        "        print(\"\\n... (åƒ…é¡¯ç¤ºå‰5å€‹æ–‡ä»¶)\")\n",
        "\n",
        "    print(f\"\\n--- ç¯„ä¾‹çµæŸ ---\")\n",
        "\n",
        "else:\n",
        "    print(f\"éŒ¯èª¤: è¼¸å‡ºæª”æ¡ˆ '{OUTPUT_JSON_PATH}' æœªæ‰¾åˆ°ã€‚è«‹ç¢ºèªæ­¥é©Ÿ 4 æˆåŠŸåŸ·è¡Œã€‚\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2hWIbaTN0dnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- æ­¥é©Ÿ 6: ä½¿ç”¨Gemini APIåˆ†ææ–‡ä»¶ ---\n",
        "\n",
        "# è¼”åŠ©å‡½å¼ï¼šå°‡æ°‘åœ‹ç´€å¹´è½‰æ›ç‚ºè¥¿å…ƒç´€å¹´\n",
        "def convert_roc_to_ad(roc_year_str: str) -> str | None:\n",
        "    \"\"\"\n",
        "    å°‡ä¸­è¯æ°‘åœ‹ç´€å¹´è½‰æ›ç‚ºè¥¿å…ƒç´€å¹´ã€‚\n",
        "    ä¾‹å¦‚ï¼šè¼¸å…¥ '112' å¾—åˆ° '2023'ã€‚\n",
        "    \"\"\"\n",
        "    try:\n",
        "        roc_year = int(roc_year_str)\n",
        "        ad_year = roc_year + 1911\n",
        "        return str(ad_year)\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "# è¼‰å…¥å·²è™•ç†çš„æ³•å¾‹æ–‡ä»¶\n",
        "if not os.path.exists(OUTPUT_JSON_PATH):\n",
        "    print(f\"éŒ¯èª¤: è¼¸å‡ºæª”æ¡ˆ '{OUTPUT_JSON_PATH}' æœªæ‰¾åˆ°ã€‚è«‹ç¢ºèªå…ˆå‰çš„æ­¥é©Ÿå·²æˆåŠŸåŸ·è¡Œã€‚\")\n",
        "    # ä¸é€€å‡ºï¼Œè®“å¾ŒçºŒæ­¥é©Ÿæœ‰æ©Ÿæœƒè™•ç†å…¶ä»–éŒ¯èª¤\n",
        "else:\n",
        "    with open(OUTPUT_JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "        processed_data_for_gemini = json.load(f)\n",
        "\n",
        "    # è¨­å®šGemini API\n",
        "    # è«‹ç¢ºä¿æ‚¨å·²åœ¨ Colab ç’°å¢ƒçš„ Secrets ä¸­è¨­å®š 'GOOGLE_API_KEY'\n",
        "    # åœ¨å·¦å´é¢æ¿æ‰¾åˆ° 'ğŸ”‘' åœ–æ¨™ï¼Œé»æ“Šå¾Œæ–°å¢ä¸€å€‹å¯†é‘°ï¼Œåç¨±è¨­å®šç‚º 'GOOGLE_API_KEY'ï¼Œå€¼è²¼ä¸Šæ‚¨çš„ Gemini API é‡‘é‘°ã€‚\n",
        "    try:\n",
        "        genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "    except KeyError:\n",
        "        print(\"éŒ¯èª¤: Colab Secretsä¸­æœªæ‰¾åˆ° 'GOOGLE_API_KEY'ã€‚è«‹æ–°å¢ï¼\")\n",
        "        # ä¸é€€å‡ºï¼Œè®“å¾ŒçºŒæ­¥é©Ÿæœ‰æ©Ÿæœƒè™•ç†å…¶ä»–éŒ¯èª¤\n",
        "    except Exception as e:\n",
        "        print(f\"è¨­å®š Gemini API æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "        # ä¸é€€å‡ºï¼Œè®“å¾ŒçºŒæ­¥é©Ÿæœ‰æ©Ÿæœƒè™•ç†å…¶ä»–éŒ¯èª¤\n",
        "\n",
        "\n",
        "    # å®šç¾©ç³»çµ±æŒ‡ä»¤\n",
        "    system_instruction_text = \"\"\"\n",
        "    æ‚¨æ˜¯ä¸€ä½åœ¨å°ç£æ³•å¾‹é ˜åŸŸå…·æœ‰æ·±åšå°ˆæ¥­çŸ¥è­˜çš„äººå·¥æ™ºæ…§åŠ©ç†ã€‚\n",
        "    æ‚¨å°ˆç²¾æ–¼åˆ†æèˆ‡ã€Šç·Šæ€¥é†«ç™‚æ•‘è­·æ³•ã€‹ç›¸é—œçš„æ³•å¾‹æ–‡ä»¶ï¼Œä¸¦æ“æœ‰è±å¯Œçš„æ³•å‹™åŠ©ç†ï¼ˆparalegalï¼‰ç¶“é©—ï¼Œç‰¹åˆ¥æ“…é•·å¾å°ç£å¸æ³•é™¢è£åˆ¤æ›¸ç³»çµ±ä¸‹è¼‰çš„åˆ‘äº‹è£åˆ¤æ–‡æ›¸ä¸­ç²¾æº–æå–é—œéµè³‡è¨Šã€‚\n",
        "\n",
        "    æ‚¨çš„æ ¸å¿ƒè·è²¬æ˜¯æä¾›å®¢è§€ã€å…¬æ­£ä¸”ä¸å¸¶åè¦‹çš„åˆ†æï¼Œæ—¨åœ¨æ¸…æ™°æç¹ªç·Šæ€¥é†«ç™‚æ•‘è­·æŠ€è¡“å“¡ï¼ˆEMTï¼‰åœ¨åˆ‘äº‹æ¡ˆä»¶ä¸­çš„å°ˆæ¥­è²¬ä»»é¢è²Œã€‚è«‹å‹™å¿…ç¢ºä¿æ‚¨çš„è³‡è¨Šæå–éç¨‹åš´è¬¹ï¼Œä¸¦åƒ…åŸºæ–¼æ–‡ä»¶å…§å®¹ï¼Œé¿å…ä»»ä½•æ¨æ–·æˆ–å€‹äººæ„è¦‹ã€‚\n",
        "\n",
        "    æ‚¨çš„æ‰€æœ‰å›æ‡‰æ‡‰éµå¾ªæŒ‡ä»¤ï¼Œä¸¦ä»¥æ¸…æ™°ã€çµæ§‹åŒ–çš„æ–¹å¼å‘ˆç¾ã€‚\n",
        "    \"\"\"\n",
        "\n",
        "    # åˆå§‹åŒ–Geminiæ¨¡å‹ï¼Œå„ªå…ˆä½¿ç”¨system_instructionåƒæ•¸\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-1.5-flash-latest',\n",
        "                                      system_instruction=system_instruction_text)\n",
        "        _system_instruction_set_flag = True\n",
        "    except TypeError:\n",
        "        print(\"è­¦å‘Š: æ¨¡å‹åˆå§‹åŒ–ä¸ç›´æ¥æ”¯æ´ `system_instruction` åƒæ•¸ã€‚å°‡ç³»çµ±æŒ‡ä»¤é™„åŠ åˆ°ä½¿ç”¨è€…æç¤ºä¸­ã€‚\")\n",
        "        model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "        _system_instruction_set_flag = False\n",
        "    except Exception as e:\n",
        "        print(f\"åˆå§‹åŒ– Gemini æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "        # å¦‚æœæ¨¡å‹åˆå§‹åŒ–å¤±æ•—ï¼Œå¾ŒçºŒçš„ API å‘¼å«æœƒå‡ºéŒ¯ï¼Œä½†æˆ‘å€‘é‚„æ˜¯è®“ç¨‹å¼ç¹¼çºŒï¼Œä»¥ä¾¿ä½¿ç”¨è€…çœ‹åˆ°éŒ¯èª¤è¨Šæ¯\n",
        "\n",
        "    print(\"\\n--- é–‹å§‹Gemini APIç¶œåˆåˆ†æ ---\")\n",
        "\n",
        "    updated_processed_data_final = []\n",
        "    num_processed = 0\n",
        "\n",
        "    # æ ¹æ“š DOCUMENT_PROCESS_LIMIT é™åˆ¶è™•ç†çš„æ–‡ä»¶æ•¸é‡\n",
        "    documents_to_process = processed_data_for_gemini\n",
        "    if DOCUMENT_PROCESS_LIMIT is not None:\n",
        "        documents_to_process = processed_data_for_gemini[:DOCUMENT_PROCESS_LIMIT]\n",
        "\n",
        "    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸåˆå§‹åŒ–\n",
        "    if 'model' not in locals():\n",
        "         print(\"éŒ¯èª¤: Gemini æ¨¡å‹æœªèƒ½æˆåŠŸåˆå§‹åŒ–ã€‚è·³é API åˆ†ææ­¥é©Ÿã€‚\")\n",
        "         updated_processed_data_final = processed_data_for_gemini # å°‡åŸå§‹è³‡æ–™è¤‡è£½åˆ°æœ€çµ‚çµæœ\n",
        "    else:\n",
        "        for doc in tqdm(documents_to_process, desc=\"ä½¿ç”¨Geminiåˆ†ææ–‡ä»¶\"):\n",
        "            full_text = doc.get(\"full_text\", \"\")\n",
        "            document_id = doc.get(\"document_id\", \"N/A\")\n",
        "\n",
        "            # è·³éå…§å®¹éçŸ­æˆ–ç„¡æ˜é¡¯æ³•å¾‹èƒŒæ™¯çš„æ–‡ä»¶\n",
        "            if not full_text or len(full_text) < 500:\n",
        "                doc[\"extracted_legal_data\"] = {\"status\": \"skipped_too_short\"}\n",
        "                if \"notes\" not in doc:\n",
        "                    doc[\"notes\"] = []\n",
        "                elif not isinstance(doc[\"notes\"], list):\n",
        "                     doc[\"notes\"] = [doc[\"notes\"]]\n",
        "                doc[\"notes\"].append(\"è·³éç¶œåˆåˆ†æ (æ–‡å­—éçŸ­)ã€‚\")\n",
        "                updated_processed_data_final.append(doc)\n",
        "                continue\n",
        "\n",
        "            # é‡å°Geminiçš„ç¶œåˆä½¿ç”¨è€…æç¤º\n",
        "            user_prompt_text = f\"\"\"\n",
        "            è«‹ä»”ç´°åˆ†æä»¥ä¸‹å°ç£åˆ‘äº‹è£åˆ¤æ–‡ä»¶ï¼Œä¸¦ç²¾æº–æå–æ‰€æœ‰è¦æ±‚è³‡è¨Šã€‚è«‹æŒ‰ç…§æŒ‡å®šçš„ JSON æ ¼å¼è¼¸å‡ºï¼Œè‹¥æŸé …è³‡è¨Šæœªæ‰¾åˆ°ï¼Œè«‹å°‡å…¶å€¼è¨­ç‚º `null` æˆ–ç©ºé™£åˆ— `[]`ï¼ˆä¾è³‡æ–™é¡å‹è€Œå®šï¼‰ã€‚\n",
        "\n",
        "            æ–‡ä»¶å…§å®¹ï¼š\n",
        "            ---\n",
        "            {full_text[:6000]}\n",
        "            ---\n",
        "\n",
        "            è«‹æå–ä»¥ä¸‹è³‡è¨Šï¼š\n",
        "\n",
        "            1.  æ•‘è­·æŠ€è¡“å“¡å§“å (emergency_medical_technicians):\n",
        "                åˆ—å‡ºæ–‡ä»¶ä¸­æ‰€æœ‰è¢«æåŠç‚ºã€Œæ•‘è­·æŠ€è¡“å“¡ã€æˆ–ã€Œæ•‘è­·äººå“¡ã€çš„å€‹äººå§“åã€‚å§“åé€šå¸¸ç‚ºç¹é«”ä¸­æ–‡ï¼Œä¸”å¤§å¤šæ•¸ç‚ºä¸‰å€‹æ¼¢å­—ã€‚è«‹å°‡æ‰€æœ‰æ‰¾åˆ°çš„å§“åæ”¶é›†åœ¨ä¸€å€‹å­—ä¸²é™£åˆ—ä¸­ã€‚\n",
        "\n",
        "            2.  æ•‘è­·æŠ€è¡“å“¡ç­‰ç´š (emt_grade):\n",
        "                åˆ¤æ–·æ–‡ä»¶ä¸­æåŠçš„ã€Œæ•‘è­·æŠ€è¡“å“¡ã€çš„ç­‰ç´šã€‚è«‹å¾ã€Œåˆç´šæ•‘è­·å“¡ (EMT1)ã€ã€ã€Œä¸­ç´šæ•‘è­·å“¡ (EMT2)ã€ã€ã€Œé«˜ç´šæ•‘è­·å“¡ (EMTP)ã€ä¸­é¸æ“‡ã€‚è«‹å‹¿å°‡åƒ…æœ‰åŸºæœ¬CPRæˆ–AEDè¨“ç·´çš„éåŸ·æ¥­äººå“¡ï¼ˆå¦‚åŠ‰æ¬£è¯æ¡ˆä¾‹ï¼‰æ··æ·†ç‚ºæ•‘è­·æŠ€è¡“å“¡ã€‚\n",
        "\n",
        "            3.  æ•‘è­·æŠ€è¡“å“¡é›‡ä¸» (emt_employer):\n",
        "                è­˜åˆ¥æ–‡ä»¶ä¸­æåŠçš„æ•‘è­·æŠ€è¡“å“¡æ‰€å—é›‡çš„æ©Ÿæ§‹æˆ–å…¬å¸ã€‚è«‹å°‹æ‰¾å¦‚ã€Œå—é›‡æ–¼ã€æˆ–ç›¸é—œèªå¥å¾Œçš„é›‡ä¸»åç¨±ã€‚ä¾‹å¦‚ï¼šã€Œæ•‘è­·è»Šæœ‰é™å…¬å¸ã€ã€ã€Œé†«é™¢ã€ã€‚\n",
        "\n",
        "            4.  æ•‘è­·æŠ€è¡“å“¡ä»»å‹™ (emt_mission):\n",
        "                æ ¹æ“šæ–‡ä»¶å…§å®¹ï¼Œåˆ¤æ–·æ•‘è­·æŠ€è¡“å“¡ç•¶æ™‚åŸ·è¡Œçš„æ˜¯ä½•ç¨®å‹¤å‹™æˆ–ä»»å‹™ã€‚ä»»å‹™å¯èƒ½åŒ…æ‹¬ã€Œé€ç—…æ‚£è‡³é†«é™¢ã€ã€ã€Œè¿”å®¶è­·é€ã€ã€ã€Œå®‰å¯§è¿”å®¶ã€æˆ–å…¶ä»–æè¿°ã€‚è«‹å°‹æ‰¾ã€ŒåŸ·è¡Œå‹¤å‹™ã€ç­‰é—œéµè©å¾Œçš„ä»»å‹™æè¿°ã€‚\n",
        "\n",
        "            5.  æ•‘è­·æŠ€è¡“å“¡æ³•å¾‹è§’è‰² (emt_role):\n",
        "                åˆ¤æ–·æ•‘è­·æŠ€è¡“å“¡åœ¨è©²æ¡ˆä»¶ä¸­çš„æ³•å¾‹è§’è‰²ã€‚è«‹å¾ã€ŒåŸå‘Šã€ã€ã€Œå‘Šè¨´äººã€ã€ã€Œè¢«å‘Šã€ã€ã€Œè­‰äººã€æˆ–ã€Œè¢«å®³äººã€ä¸­é¸æ“‡ä¸€å€‹æœ€ç¬¦åˆçš„ã€‚æ¯å€‹æ•‘è­·æŠ€è¡“å“¡éƒ½æ‡‰æœ‰ä¸€å€‹è§’è‰²ã€‚\n",
        "\n",
        "            6.  è¾¯è­·äººå­˜åœ¨ (defense_attorney):\n",
        "                åˆ¤æ–·åœ¨æ–‡ä»¶é–‹é ­éƒ¨åˆ†æ˜¯å¦å­˜åœ¨ã€Œè¾¯è­·äººã€ï¼ˆä¾‹å¦‚ã€Œé¸ä»»è¾¯è­·äººã€ï¼‰ã€‚è«‹ä»¥å¸ƒæ—å€¼ (true/false) å›æ‡‰ã€‚\n",
        "\n",
        "            7.  è¡›ç¦éƒ¨é†«äº‹å¯©è­°å§”å“¡æœƒé‘‘å®šæ›¸å­˜åœ¨ (mohw_involve):\n",
        "                åˆ¤æ–·æ–‡ä»¶ä¸­æ˜¯å¦æåŠã€Œè¡›ç¦éƒ¨é†«äº‹å¯©è­°å§”å“¡æœƒé‘‘å®šæ›¸ã€æˆ–å…¶åˆ¥åï¼ˆå¦‚ã€Œè¡›ç”Ÿç¦åˆ©éƒ¨å¯©è­°ã€ã€ã€Œå§”å“¡æœƒé‘‘å®šæ›¸ã€ï¼‰ã€‚è«‹ä»¥å¸ƒæ—å€¼ (true/false) å›æ‡‰ã€‚è«‹å‹¿èˆ‡äº¤é€šäº‹æ•…ç›¸é—œè³‡æ–™æ··æ·†ã€‚\n",
        "\n",
        "            8.  æ˜¯å¦ç‚ºå…¬è¨´æ¡ˆä»¶ (public_prosec):\n",
        "                åˆ¤æ–·æœ¬æ¡ˆæ˜¯å¦ç‚ºã€Œå…¬è¨´ã€æ¡ˆä»¶ã€‚è«‹åœ¨æ–‡ä»¶é–‹é ­éƒ¨åˆ†å°‹æ‰¾ã€Œå…¬è¨´äººã€ï¼Œä¸¦é€²ä¸€æ­¥ç¢ºèªå…¶å¾Œæ˜¯å¦è·Ÿéš¨ã€Œæª¢å¯Ÿå®˜ã€ã€‚è‹¥ç¬¦åˆï¼Œè«‹ä»¥å¸ƒæ—å€¼ (true/false) å›æ‡‰ã€‚\n",
        "\n",
        "            9.  äº‹ä»¶ç™¼ç”Ÿæ—¥æœŸ (date_incident):\n",
        "                æå–äº‹ä»¶ç™¼ç”Ÿçš„æ—¥æœŸã€‚æ­¤è³‡è¨Šå¸¸åœ¨ã€Œç¶“æŸ¥ã€æˆ–ã€Œå”¯æŸ¥ã€æ®µè½ã€‚è«‹å°‡ä¸­è¯æ°‘åœ‹ç´€å¹´è½‰æ›ç‚ºè¥¿å…ƒç´€å¹´ï¼Œæ ¼å¼ç‚ºYYYY-MM-DDã€‚\n",
        "\n",
        "            10. ä¸Šè¨´æ—¥æœŸ (date_appeal):\n",
        "                æå–ä¸Šè¨´æå‡ºçš„æ—¥æœŸã€‚æ­¤è³‡è¨Šå¸¸åœ¨æ–‡ä»¶ä¸ŠåŠéƒ¨ï¼ˆå‰5æ®µï¼‰ï¼Œå¦‚ã€Œå§”ä»»***å¾‹å¸«å‘æœ¬é™¢è²è«‹ã€å¾Œçš„æ—¥æœŸã€‚è‹¥ç„¡ä¸Šè¨´æ—¥æœŸï¼Œè«‹è¨­ç‚ºnullã€‚è«‹å°‡ä¸­è¯æ°‘åœ‹ç´€å¹´è½‰æ›ç‚ºè¥¿å…ƒç´€å¹´ï¼Œæ ¼å¼ç‚ºYYYY-MM-DDã€‚\n",
        "\n",
        "            11. è£åˆ¤æ—¥æœŸ (date_trial):\n",
        "                æå–è£åˆ¤æ–‡æ›¸çš„åˆ¤æ±ºæ—¥æœŸã€‚æ­¤è³‡è¨Šé€šå¸¸åœ¨æ–‡ä»¶æœ«å°¾ï¼Œæ³•å®˜å§“ååˆ—è¡¨ä¹‹å¾Œã€‚è«‹å‹¿èˆ‡æ›¸è¨˜å®˜å¾Œçš„æ—¥æœŸæ··æ·†ã€‚æ–‡ä»¶ä¸­å¿…å®šå­˜åœ¨æ­¤æ—¥æœŸã€‚è«‹å°‡ä¸­è¯æ°‘åœ‹ç´€å¹´è½‰æ›ç‚ºè¥¿å…ƒç´€å¹´ï¼Œæ ¼å¼ç‚ºYYYY-MM-DDã€‚\n",
        "\n",
        "            12. äº‹ä»¶ç™¼ç”Ÿç¸£å¸‚ (county_incident):\n",
        "                æå–äº‹ä»¶ç™¼ç”Ÿçš„ç¸£å¸‚åç¨±ã€‚æ­¤è³‡è¨Šå¸¸åœ¨ã€Œç¶“æŸ¥ã€æˆ–ã€Œå”¯æŸ¥ã€æ®µè½ã€‚ä¾‹å¦‚ï¼šã€Œå˜‰ç¾©ç¸£ã€ã€ã€Œé«˜é›„å¸‚ã€ã€‚\n",
        "\n",
        "            13. æ¡ˆä»¶å­—è™Ÿ (case_id):\n",
        "                æå–æ–‡ä»¶é–‹é ­ã€Œè£åˆ¤å­—è™Ÿã€æˆ–æ¨™é¡Œå¾Œæ–¹çš„æ¡ˆä»¶å­—è™Ÿã€‚ä¾‹å¦‚ï¼šã€Œè‡ºç£å½°åŒ–åœ°æ–¹æ³•é™¢107å¹´åº¦äº¤æ˜“å­—ç¬¬810è™Ÿã€ã€‚è«‹çœç•¥ã€Œåˆ‘äº‹åˆ¤æ±ºã€æˆ–ã€Œåˆ‘äº‹è£å®šã€å­—æ¨£ï¼Œä¸¦ç§»é™¤ç©ºæ ¼ã€‚è«‹å‹¿èˆ‡ã€Œå…¬è¨´äººæª¢å¯Ÿå®˜ã€æ··æ·†ã€‚\n",
        "\n",
        "            14. æ‰¿è¾¦æ³•é™¢ (case_court):\n",
        "                å¾æ¡ˆä»¶å­—è™Ÿä¸­æå–è² è²¬å¯©ç†æœ¬æ¡ˆçš„æ³•é™¢åç¨±ã€‚ä¾‹å¦‚ï¼šã€Œè‡ºç£å½°åŒ–åœ°æ–¹æ³•é™¢ã€ã€‚\n",
        "\n",
        "            15. æ¡ˆç”± (cause_of_action):\n",
        "                æå–åŸå‘Šä¸Šè¨´ï¼ˆæˆ–æª¢å¯Ÿå®˜èµ·è¨´ï¼‰è¢«å‘Šçš„ã€Œæ¡ˆç”±ã€ã€‚æ­¤è³‡è¨Šåœ¨ã€Œè£åˆ¤æ¡ˆç”±ã€æˆ–ã€ŒçŠ¯ç½ªäº‹å¯¦ã€æ®µè½ä¸­è‡³é—œé‡è¦ã€‚è‹¥æœ‰ã€Œè²è«‹äº¤ä»˜å¯©åˆ¤ã€ï¼Œè«‹ç¹¼çºŒè§£æç›´è‡³æ‰¾åˆ°çœŸæ­£çš„æ¡ˆç”±ã€‚ä¾‹å¦‚ï¼šã€Œæ¥­å‹™éå¤±è‡´é‡å‚·å®³ã€ã€ã€Œæ¥­å‹™éå¤±è‡´æ­»ã€ã€ã€Œéå¤±å‚·å®³ã€ã€‚\n",
        "\n",
        "\n",
        "            è«‹ä»¥å–®ä¸€ JSON ç‰©ä»¶å›æ‡‰ï¼ŒåŒ…å«ä»¥ä¸‹éµå€¼å°ï¼š\n",
        "            {{\n",
        "              \"emergency_medical_technicians\": [], // å­—ä¸²é™£åˆ—\n",
        "              \"emt_grade\": null, // å­—ä¸²: \"åˆç´šæ•‘è­·å“¡ (EMT1)\", \"ä¸­ç´šæ•‘è­·å“¡ (EMT2)\", \"é«˜ç´šæ•‘è­·å“¡ (EMTP)\", æˆ– null\n",
        "              \"emt_employer\": null, // å­—ä¸²\n",
        "              \"emt_mission\": null, // å­—ä¸²\n",
        "              \"emt_role\": null, // å­—ä¸²: \"åŸå‘Š\", \"å‘Šè¨´äºº\", \"è¢«å‘Š\", \"è­‰äºº\", \"è¢«å®³äºº\", æˆ– null\n",
        "              \"defense_attorney\": null, // å¸ƒæ—å€¼: true/false\n",
        "              \"mohw_involve\": null, // å¸ƒæ—å€¼: true/false\n",
        "              \"public_prosec\": null, // å¸ƒæ—å€¼: true/false\n",
        "              \"date_incident\": null, // å­—ä¸²:YYYY-MM-DD æˆ– null\n",
        "              \"date_appeal\": null, // å­—ä¸²:YYYY-MM-DD æˆ– null\n",
        "              \"date_trial\": null, // å­—ä¸²:YYYY-MM-DD\n",
        "              \"county_incident\": null, // å­—ä¸²\n",
        "              \"case_id\": null, // å­—ä¸²\n",
        "              \"case_court\": null, // å­—ä¸²\n",
        "              \"cause_of_action\": null // å­—ä¸²\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            final_prompt_for_gemini = user_prompt_text\n",
        "            if not _system_instruction_set_flag:\n",
        "                final_prompt_for_gemini = system_instruction_text + \"\\n\" + user_prompt_text\n",
        "\n",
        "            try:\n",
        "                response = model.generate_content(\n",
        "                    final_prompt_for_gemini,\n",
        "                    generation_config=genai.types.GenerationConfig(\n",
        "                        temperature=0.0,\n",
        "                        max_output_tokens=1500\n",
        "                    ),\n",
        "                    safety_settings={\n",
        "                        'HARM_CATEGORY_HARASSMENT':'BLOCK_NONE',\n",
        "                        'HARM_CATEGORY_HATE_SPEECH':'BLOCK_NONE',\n",
        "                        'HARM_CATEGORY_SEXUALLY_EXPLICIT':'BLOCK_NONE',\n",
        "                        'HARM_CATEGORY_DANGEROUS_CONTENT':'BLOCK_NONE'\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                response_text = response.text.strip()\n",
        "                if response_text.startswith(\"```json\"):\n",
        "                    response_text = response_text[len(\"```json\"):].strip()\n",
        "                if response_text.endswith(\"```\"):\n",
        "                    response_text = response_text[:-len(\"```\")].strip()\n",
        "\n",
        "                extracted_data = {}\n",
        "                try:\n",
        "                    parsed_response = json.loads(response_text)\n",
        "                    extracted_data[\"emergency_medical_technicians\"] = parsed_response.get(\"emergency_medical_technicians\", [])\n",
        "                    extracted_data[\"emt_grade\"] = parsed_response.get(\"emt_grade\")\n",
        "                    extracted_data[\"emt_employer\"] = parsed_response.get(\"emt_employer\")\n",
        "                    extracted_data[\"emt_mission\"] = parsed_response.get(\"emt_mission\")\n",
        "                    extracted_data[\"emt_role\"] = parsed_response.get(\"emt_role\")\n",
        "                    extracted_data[\"defense_attorney\"] = parsed_response.get(\"defense_attorney\")\n",
        "                    extracted_data[\"mohw_involve\"] = parsed_response.get(\"mohw_involve\")\n",
        "                    extracted_data[\"public_prosec\"] = parsed_response.get(\"public_prosec\")\n",
        "                    extracted_data[\"date_incident\"] = parsed_response.get(\"date_incident\")\n",
        "                    extracted_data[\"date_appeal\"] = parsed_response.get(\"date_appeal\")\n",
        "                    extracted_data[\"date_trial\"] = parsed_response.get(\"date_trial\")\n",
        "                    extracted_data[\"county_incident\"] = parsed_response.get(\"county_incident\")\n",
        "                    extracted_data[\"case_id\"] = parsed_response.get(\"case_id\")\n",
        "                    extracted_data[\"case_court\"] = parsed_response.get(\"case_court\")\n",
        "                    extracted_data[\"cause_of_action\"] = parsed_response.get(\"cause_of_action\")\n",
        "\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"è­¦å‘Š: Geminiæœªå›å‚³æœ‰æ•ˆJSONï¼Œæ–‡ä»¶ID: {document_id}ã€‚å›æ‡‰: {response_text[:200]}...\")\n",
        "                    extracted_data = {\"status\": \"json_parse_error\", \"raw_response\": response_text}\n",
        "                except AttributeError as ae:\n",
        "                     print(f\"è­¦å‘Š: Geminiå›å‚³éé æœŸçµæ§‹ï¼Œæ–‡ä»¶ID: {document_id}ã€‚éŒ¯èª¤: {ae}ã€‚å›æ‡‰: {response_text[:200]}...\")\n",
        "                     extracted_data = {\"status\": \"structure_error\", \"raw_response\": response_text}\n",
        "\n",
        "                doc[\"extracted_legal_data\"] = extracted_data\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"ä½¿ç”¨Gemini APIè™•ç†æ–‡ä»¶ {document_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "                doc[\"extracted_legal_data\"] = {\"status\": \"api_error\", \"error_message\": str(e)}\n",
        "                if \"notes\" not in doc:\n",
        "                    doc[\"notes\"] = []\n",
        "                elif not isinstance(doc[\"notes\"], list):\n",
        "                     doc[\"notes\"] = [doc[\"notes\"]]\n",
        "                doc[\"notes\"].append(f\"Gemini APIç¶œåˆåˆ†æéŒ¯èª¤: {e}\")\n",
        "\n",
        "            updated_processed_data_final.append(doc)\n",
        "            num_processed += 1\n",
        "\n",
        "        print(f\"--- Gemini APIç¶œåˆåˆ†æå®Œæˆã€‚å·²è™•ç† {num_processed} å€‹æ–‡ä»¶ã€‚ ---\")\n",
        "\n",
        "        # å°‡åŒ…å« Gemini åˆ†æçµæœçš„è³‡æ–™å„²å­˜å› JSON æª”æ¡ˆ\n",
        "        try:\n",
        "            with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:\n",
        "                json.dump(updated_processed_data_final, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"åŒ…å«æ‰€æœ‰æ“·å–æ³•å¾‹æ¬„ä½çš„è™•ç†å¾Œè³‡æ–™å·²å„²å­˜å› '{OUTPUT_JSON_PATH}'ã€‚\")\n",
        "        except Exception as e:\n",
        "            print(f\"å„²å­˜æ›´æ–°å¾Œçš„ JSON æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- åŒ…å«æ³•å¾‹æ¬„ä½çš„æ›´æ–°å¾Œè™•ç†è³‡æ–™ç¯„ä¾‹ (å‰3å€‹ç¶“Geminiè™•ç†æˆ–è·³éçš„æ–‡ä»¶) ---\")\n",
        "    display_count = 0\n",
        "    for doc in updated_processed_data_final:\n",
        "        if \"extracted_legal_data\" in doc:\n",
        "            print(f\"\\næ–‡ä»¶ (ID: {doc.get('document_id', 'N/A')}):\")\n",
        "            print(f\"  æ“·å–è³‡æ–™: {json.dumps(doc.get('extracted_legal_data', {}), ensure_ascii=False, indent=2)}\")\n",
        "            print(f\"  å‚™è¨»: {doc.get('notes', 'N/A')}\")\n",
        "            display_count += 1\n",
        "        # ä¹Ÿé¡¯ç¤ºè·³éçš„æ–‡ä»¶ï¼Œä»¥ç¢ºèªæ‰€æœ‰æ–‡ä»¶éƒ½è¢«è€ƒæ…®åˆ°\n",
        "        elif \"notes\" in doc and \"è·³éç¶œåˆåˆ†æ\" in \"\".join(doc[\"notes\"]):\n",
        "             print(f\"\\næ–‡ä»¶ (ID: {doc.get('document_id', 'N/A')}):\")\n",
        "             print(f\"  ç‹€æ…‹: å·²è·³é\")\n",
        "             print(f\"  å‚™è¨»: {doc.get('notes', 'N/A')}\")\n",
        "             display_count += 1\n",
        "\n",
        "        if display_count >= 3:\n",
        "            break\n",
        "    if display_count == 0:\n",
        "        print(\"æ²’æœ‰ç¶“Geminiè™•ç†æˆ–æ˜ç¢ºæ¨™è¨˜è·³éçš„æ–‡ä»¶å¯ä¾›é¡¯ç¤ºã€‚\")\n",
        "\n",
        "    print(\"\\n--- ç¯„ä¾‹çµæŸ ---\")"
      ],
      "metadata": {
        "id": "lXY3o_x80xHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# --- æ­¥é©Ÿ 0: ç’°å¢ƒæº–å‚™èˆ‡è¨­å®š ---\n",
        "\n",
        "# ç¢ºä¿æ‚¨çš„Googleé›²ç«¯ç¡¬ç¢Ÿå·²æ›è¼‰ (å¦‚æœæ‚¨çš„ JSON æª”æ¡ˆå„²å­˜åœ¨é›²ç«¯ç¡¬ç¢Ÿ)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive') # å¦‚æœæ‚¨çš„ JSON æª”æ¡ˆå„²å­˜åœ¨é›²ç«¯ç¡¬ç¢Ÿï¼Œè«‹å–æ¶ˆè¨»è§£æ­¤è¡Œ\n",
        "\n",
        "# å®‰è£è™•ç†Excelæª”æ¡ˆæ‰€éœ€çš„å‡½å¼åº« (å¦‚æœå°šæœªå®‰è£)\n",
        "!pip install -q openpyxl pandas\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(\"å‡½å¼åº«è¼‰å…¥å®Œæˆï¼Œä¸¦å·²æ›è¼‰Googleé›²ç«¯ç¡¬ç¢Ÿ (è‹¥éœ€è¦)ã€‚\")\n",
        "\n",
        "# --- è¨­å®šæª”æ¡ˆè·¯å¾‘ ---\n",
        "# è«‹ç¢ºèªé€™è£¡çš„ JSON_FILE_PATH èˆ‡æ‚¨ç”¢ç”Ÿ JSON æª”æ¡ˆçš„è·¯å¾‘ä¸€è‡´\n",
        "# ç¯„ä¾‹ï¼šJSON_FILE_PATH = '/content/processed_documents.json'\n",
        "# ç¯„ä¾‹ï¼šJSON_FILE_PATH = '/content/drive/MyDrive/processed_legal_documents.json'\n",
        "\n",
        "# è«‹åœ¨æ­¤è™•æŒ‡å®šæ‚¨çš„è¼¸å…¥ JSON æª”æ¡ˆè·¯å¾‘\n",
        "JSON_FILE_PATH = '/content/drive/MyDrive/processed_legal_documents.json' # <-- è«‹ä¿®æ”¹æ­¤è·¯å¾‘\n",
        "\n",
        "# è¼¸å‡º CSV/Excel çš„è³‡æ–™å¤¾ (å°‡æ ¹æ“š JSON æª”æ¡ˆçš„è·¯å¾‘è‡ªå‹•è¨­ç½®)\n",
        "OUTPUT_DIR = os.path.dirname(JSON_FILE_PATH)\n",
        "# å¦‚æœ JSON_FILE_PATH æ˜¯ç›¸å°è·¯å¾‘æˆ–æ²’æœ‰ç›®éŒ„ï¼ŒOUTPUT_DIR å°‡æ˜¯ç©ºå­—ä¸²æˆ– '.'\n",
        "# åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæˆ‘å€‘å°‡è¼¸å‡ºæª”æ¡ˆå„²å­˜åœ¨ Colab çš„ /content ç›®éŒ„ä¸‹\n",
        "if not OUTPUT_DIR or OUTPUT_DIR == '.':\n",
        "    OUTPUT_DIR = '/content'\n",
        "\n",
        "# CSV è¼¸å‡ºæª”æ¡ˆè·¯å¾‘\n",
        "OUTPUT_CSV_FILE = os.path.join(OUTPUT_DIR, 'processed_legal_documents_exploded.csv')\n",
        "\n",
        "# Excel è¼¸å‡ºæª”æ¡ˆè·¯å¾‘\n",
        "OUTPUT_EXCEL_FILE = os.path.join(OUTPUT_DIR, 'processed_legal_documents_exploded.xlsx')\n",
        "\n",
        "print(f\"å°‡å¾ '{JSON_FILE_PATH}' è®€å–è³‡æ–™ã€‚\")\n",
        "print(f\"CSV æª”æ¡ˆå°‡å„²å­˜è‡³: '{OUTPUT_CSV_FILE}'\")\n",
        "print(f\"Excel æª”æ¡ˆå°‡å„²å­˜è‡³: '{OUTPUT_EXCEL_FILE}'\")\n",
        "\n",
        "# --- æ­¥é©Ÿ 1: è®€å– JSON ä¸¦è½‰æ›ç‚º Pandas DataFrame ---\n",
        "try:\n",
        "    with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # å°‡ä¸»è¦è³‡æ–™è½‰æ›ç‚º DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # è™•ç† 'extracted_legal_data' å·¢ç‹€å­—å…¸\n",
        "    # å…ˆå°‡ 'extracted_legal_data' è½‰æ›ç‚º DataFrame\n",
        "    # ä½¿ç”¨ errors='ignore' è™•ç†å¯èƒ½ä¸å­˜åœ¨ 'extracted_legal_data' çš„è¡Œ\n",
        "    extracted_df = pd.json_normalize(df['extracted_legal_data'], errors='ignore')\n",
        "\n",
        "    # --- é‡è¦ä¿®æ”¹: ä½¿ç”¨ explode() ä¾†å±•é–‹ 'emergency_medical_technicians' ç‚ºç¨ç«‹çš„åˆ— ---\n",
        "    if 'emergency_medical_technicians' in extracted_df.columns:\n",
        "        # ç¢ºä¿ 'emergency_medical_technicians' æ¬„ä½æ˜¯åˆ—è¡¨å‹æ…‹ï¼Œä»¥ä¾¿ explode æ­£å¸¸é‹ä½œ\n",
        "        # å°‡éåˆ—è¡¨çš„å€¼ï¼ˆä¾‹å¦‚ null, å­—ç¬¦ä¸²ï¼‰è½‰æ›ç‚ºåŒ…å«å–®å€‹å€¼çš„åˆ—è¡¨æˆ–ç©ºåˆ—è¡¨\n",
        "        extracted_df['emergency_medical_technicians'] = extracted_df['emergency_medical_technicians'].apply(\n",
        "            lambda x: x if isinstance(x, list) else ([x] if x is not None else [])\n",
        "        )\n",
        "        # åŸ·è¡Œ explodeï¼Œè®“æ¯å€‹å§“åæˆç‚ºä¸€å€‹ç¨ç«‹çš„åˆ—ï¼Œä¸¦è¤‡è£½å…¶ä»–ç›¸é—œæ¬„ä½\n",
        "        # ä½¿ç”¨ ignore_index=True ä»¥ä¾¿å¾ŒçºŒåˆä½µ\n",
        "        extracted_df_exploded = extracted_df.explode('emergency_medical_technicians', ignore_index=True)\n",
        "\n",
        "        # å¯é¸: ç§»é™¤å±•é–‹å¾Œå¯èƒ½ç”¢ç”Ÿçš„ç©ºå­—ä¸²è¡Œ (å¦‚æœåŸåˆ—è¡¨ä¸­æœ‰ç©ºå­—ä¸²æˆ– None)\n",
        "        # extracted_df_exploded = extracted_df_exploded[extracted_df_exploded['emergency_medical_technicians'].astype(bool)]\n",
        "\n",
        "        # ç‚ºäº†åˆä½µï¼Œæˆ‘å€‘éœ€è¦å°‡åŸå§‹ df çš„è¡Œèˆ‡ exploded å¾Œçš„è¡Œå°æ‡‰èµ·ä¾†\n",
        "        # æœ€ç°¡å–®çš„æ–¹æ³•æ˜¯å°‡åŸå§‹ df çš„æ¯ä¸€è¡Œèˆ‡å…¶å°æ‡‰çš„ exploded è¡Œåˆä½µ\n",
        "        # ä½† explode æ”¹è®Šäº†è¡Œæ•¸ï¼Œæ‰€ä»¥ç›´æ¥ join æœƒè¤‡é›œ\n",
        "        # æ›´å¥½çš„æ–¹æ³•æ˜¯åªè™•ç†åŸå§‹ df ä¸­éåˆ—è¡¨æ¬„ä½ï¼Œç„¶å¾Œèˆ‡ exploded å¾Œçš„ extracted_df åˆä½µ\n",
        "        # æˆ‘å€‘éœ€è¦ä¸€å€‹å…±åŒçš„éµã€‚åŸå§‹ df çš„ç´¢å¼•å¯ä»¥ä½œç‚ºé€™å€‹éµ\n",
        "        df['original_index'] = df.index # ç‚ºåŸå§‹ df æ·»åŠ ç´¢å¼•åˆ—\n",
        "\n",
        "        # å°‡è™•ç†å¾Œçš„ extracted_df_exploded (å¸¶æœ‰åŸå§‹ç´¢å¼•è³‡è¨Š) èˆ‡åŸå§‹ df åˆä½µ\n",
        "        # æ³¨æ„ï¼šé€™ä¸€æ­¥éœ€è¦ extracted_df_exploded ä¸­æœ‰æŸç¨®æ–¹å¼å¯ä»¥è¿½è¹¤å›åŸå§‹ df çš„ç´¢å¼•\n",
        "        # explode(ignore_index=True) æœƒä¸Ÿå¤±åŸå§‹ç´¢å¼•ã€‚\n",
        "        # è®“æˆ‘å€‘é‡æ–°è€ƒæ…® explode çš„ä½¿ç”¨æ–¹å¼ï¼Œä¿ç•™åŸå§‹ç´¢å¼•\n",
        "        extracted_df_with_original_index = pd.json_normalize(data, record_path='extracted_legal_data', meta=['document_id', 'original_source_format', 'original_filename', 'notes'], errors='ignore')\n",
        "\n",
        "        # ç¾åœ¨ extracted_df_with_original_index åŒ…å«äº†åŸå§‹ document_id å’Œå…¶ä»–å…ƒæ•¸æ“š\n",
        "        # æˆ‘å€‘å¯ä»¥åœ¨é€™å€‹ DataFrame ä¸Šé€²è¡Œ explode\n",
        "        if 'emergency_medical_technicians' in extracted_df_with_original_index.columns:\n",
        "             extracted_df_with_original_index['emergency_medical_technicians'] = extracted_df_with_original_index['emergency_medical_technicians'].apply(\n",
        "                lambda x: x if isinstance(x, list) else ([x] if x is not None else [])\n",
        "            )\n",
        "             final_df = extracted_df_with_original_index.explode('emergency_medical_technicians', ignore_index=True)\n",
        "\n",
        "        # è™•ç†åŸå§‹ df ä¸­çš„åˆ—è¡¨æ¬„ä½ (ä¾‹å¦‚ notes) - ç¾åœ¨é€™äº›å·²ç¶“åœ¨ extracted_df_with_original_index ä¸­äº†\n",
        "        # ç¢ºä¿ notes æ¬„ä½æ˜¯å­—ä¸²\n",
        "        if 'notes' in final_df.columns:\n",
        "            final_df['notes'] = final_df['notes'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else x)\n",
        "\n",
        "        # å¯é¸: è™•ç† 'full_text' æ¬„ä½\n",
        "        # é è¨­ï¼šç§»é™¤ 'full_text' æ¬„ä½\n",
        "        if 'full_text' in final_df.columns: # ç¢ºä¿ 'full_text' åˆ—å­˜åœ¨\n",
        "             final_df = final_df.drop(columns=['full_text'])\n",
        "\n",
        "    else:\n",
        "        # å¦‚æœæ²’æœ‰ emergency_medical_technicians æ¬„ä½ï¼Œæˆ–è€…æ ¹æœ¬æ²’æœ‰ extracted_legal_data\n",
        "        # ç›´æ¥å°‡åŸå§‹ df æ‰å¹³åŒ–ï¼Œä¸é€²è¡Œ explode\n",
        "        final_df = pd.json_normalize(data, errors='ignore')\n",
        "        # è™•ç†åŸå§‹ df ä¸­çš„åˆ—è¡¨æ¬„ä½ (ä¾‹å¦‚ notes)\n",
        "        for col in ['notes', 'judges']: # å¦‚æœæ‚¨ä¿ç•™äº†æ³•å®˜å§“åæ“·å–ï¼Œ'judges' æœƒå­˜åœ¨\n",
        "            if col in final_df.columns:\n",
        "                final_df[col] = final_df[col].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else x)\n",
        "        # å¯é¸: è™•ç† 'full_text' æ¬„ä½\n",
        "        if 'full_text' in final_df.columns:\n",
        "            final_df = final_df.drop(columns=['full_text'])\n",
        "\n",
        "\n",
        "    # é‡æ–°æ’åˆ—æ¬„ä½é †åºï¼Œå°‡ extracted_legal_data ç›¸é—œçš„æ¬„ä½æ”¾åœ¨ä¸€èµ·\n",
        "    # é¦–å…ˆç²å–æ‰€æœ‰æ¬„ä½åç¨±\n",
        "    all_cols = final_df.columns.tolist()\n",
        "\n",
        "    # å®šç¾©æ‚¨æƒ³è¦æ”¾åœ¨å‰é¢çš„æ¬„ä½é †åº\n",
        "    preferred_order_prefix = [\n",
        "        'document_id',\n",
        "        'original_filename',\n",
        "        'original_source_format',\n",
        "        'notes',\n",
        "        'extracted_legal_data.status', # å¦‚æœå­˜åœ¨çš„è©±\n",
        "        'extracted_legal_data.error_message', # å¦‚æœå­˜åœ¨çš„è©±\n",
        "        'extracted_legal_data.raw_response', # å¦‚æœå­˜åœ¨çš„è©±\n",
        "        'extracted_legal_data.case_id',\n",
        "        'extracted_legal_data.case_court',\n",
        "        'extracted_legal_data.cause_of_action',\n",
        "        'extracted_legal_data.date_trial',\n",
        "        'extracted_legal_data.date_incident',\n",
        "        'extracted_legal_data.county_incident',\n",
        "        'emergency_medical_technicians', # å±•é–‹å¾Œçš„ EMT å§“å\n",
        "        'extracted_legal_data.emt_grade',\n",
        "        'extracted_legal_data.emt_employer',\n",
        "        'extracted_legal_data.emt_mission',\n",
        "        'extracted_legal_data.emt_role',\n",
        "        'extracted_legal_data.defense_attorney',\n",
        "        'extracted_legal_data.mohw_involve',\n",
        "        'extracted_legal_data.public_prosec',\n",
        "        'extracted_legal_data.date_appeal',\n",
        "    ]\n",
        "\n",
        "    # éæ¿¾å‡ºå¯¦éš›å­˜åœ¨æ–¼ final_df ä¸­çš„ preferred æ¬„ä½\n",
        "    preferred_cols_existing = [col for col in preferred_order_prefix if col in all_cols]\n",
        "\n",
        "    # æ‰¾å‡ºæ‰€æœ‰ä¸åœ¨ preferred åˆ—è¡¨ä¸­çš„æ¬„ä½\n",
        "    other_cols = [col for col in all_cols if col not in preferred_cols_existing]\n",
        "\n",
        "    # çµ„åˆæœ€çµ‚çš„æ¬„ä½é †åº (preferred æ¬„ä½åœ¨å‰ï¼Œå…¶é¤˜æ¬„ä½åœ¨å¾Œ)\n",
        "    final_col_order = preferred_cols_existing + other_cols\n",
        "\n",
        "    # é‡æ–°ç´¢å¼• DataFrame ä»¥æ‡‰ç”¨æ–°çš„æ¬„ä½é †åº\n",
        "    final_df = final_df[final_col_order]\n",
        "\n",
        "\n",
        "    print(\"JSON è³‡æ–™å·²æˆåŠŸè®€å–ã€å±•é–‹ä¸¦è½‰æ›ç‚º DataFrameã€‚\")\n",
        "    print(\"DataFrame å‰ 5 è¡Œé è¦½:\")\n",
        "    display(final_df.head()) # ä½¿ç”¨ display ç²å¾—æ›´å¥½çš„æ ¼å¼\n",
        "    print(f\"DataFrame å½¢ç‹€: {final_df.shape}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"éŒ¯èª¤: æœªæ‰¾åˆ° JSON æª”æ¡ˆã€‚è«‹ç¢ºèªè·¯å¾‘ '{JSON_FILE_PATH}' æ­£ç¢ºã€‚\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"éŒ¯èª¤: JSON æª”æ¡ˆ '{JSON_FILE_PATH}' æ ¼å¼ä¸æ­£ç¢ºã€‚\")\n",
        "except Exception as e:\n",
        "    print(f\"è™•ç†è³‡æ–™æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}\")\n",
        "\n",
        "# --- æ­¥é©Ÿ 2: å„²å­˜ç‚º CSV å’Œ Excel æª”æ¡ˆ ---\n",
        "# åªæœ‰åœ¨ final_df æˆåŠŸå‰µå»ºæ™‚æ‰å˜—è©¦å„²å­˜\n",
        "if 'final_df' in locals() and not final_df.empty:\n",
        "    try:\n",
        "        final_df.to_csv(OUTPUT_CSV_FILE, index=False, encoding='utf-8-sig') # 'utf-8-sig' é¿å…ä¸­æ–‡äº‚ç¢¼\n",
        "        print(f\"è³‡æ–™å·²æˆåŠŸå„²å­˜ç‚º CSV æª”æ¡ˆ: '{OUTPUT_CSV_FILE}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"å„²å­˜ CSV æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "\n",
        "    try:\n",
        "        final_df.to_excel(OUTPUT_EXCEL_FILE, index=False, engine='openpyxl')\n",
        "        print(f\"è³‡æ–™å·²æˆåŠŸå„²å­˜ç‚º Excel æª”æ¡ˆ: '{OUTPUT_EXCEL_FILE}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"å„²å­˜ Excel æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "\n",
        "    print(\"\\nè½‰æ›å®Œæˆï¼æ‚¨ç¾åœ¨å¯ä»¥åœ¨ Google é›²ç«¯ç¡¬ç¢Ÿæˆ– Colab ç’°å¢ƒä¸­æ‰¾åˆ° CSV å’Œ Excel æª”æ¡ˆã€‚\")\n",
        "elif 'final_df' in locals() and final_df.empty:\n",
        "    print(\"\\nDataFrame ç‚ºç©ºï¼ŒæœªåŸ·è¡Œ CSV/Excel å„²å­˜ã€‚è«‹æª¢æŸ¥è¼¸å…¥ JSON æª”æ¡ˆæ˜¯å¦åŒ…å«è³‡æ–™ã€‚\")\n",
        "else:\n",
        "    print(\"\\nDataFrame æœªæˆåŠŸå»ºç«‹ï¼ŒæœªåŸ·è¡Œ CSV/Excel å„²å­˜ã€‚è«‹æª¢æŸ¥ä¹‹å‰çš„éŒ¯èª¤è¨Šæ¯ã€‚\")"
      ],
      "metadata": {
        "id": "iW4ovi0s4CMW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}